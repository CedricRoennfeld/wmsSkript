% !TeX root = ../script.tex

\newpage
\section{Lösungen}

\refstepcounter{subsection}
\begin{loesung}
Für die Verteilung von $Y_n$ gilt:

\begin{align*}
    F_{Y_n}(y)
    &= \P(Y_n\leq y) 
    = \P(\max\{X_1,\dots,X_n\}\leq y+\ln n) 
    = \P(X_i\leq y+\ln n, \quad\forall i=1,\dots,n)  \\
    &= \P(X_1\leq y+\ln n)^n
    = \One_{(0,\infty)}(y)\cdot(1-e^{-(y+\ln n)})^n
    = \One_{(0,\infty)}(y)\cdot\left(1-\dfrac{e^{-y}}{n}\right)^n
\end{align*}

Diese Verteilung ist überall stetig und für den Grenzwert gilt

\[
    \lim_{n\to\infty} F_{Y_n}(y) = \One_{(0,\infty)}(y)\cdot\exp(-e^{-y})
\]
\end{loesung}

\begin{loesung}
\begin{enumerate}
    \item[a)] Aus der Tschebyscheffschen Ungleichung folgt für $p=2$, dass 
    \begin{align*}  
        \P(|X_n - a| \geq \varepsilon) 
        &\leq \dfrac{\E(X_n-a)^2}{\varepsilon^2}  \\
        &= \dfrac{\E(X_n)^2-2a\E X_n+a^2}{\varepsilon^2} \\
        &= \dfrac{(\E X_n)^2-\Var X_n - 2a\E X_n+a^2}{\varepsilon^2} \\
        &\to \dfrac{a^2-0-2a^2+a^2}{\varepsilon^2} = 0
    \end{align*}
    und damit folgt $X_n\conP a$.
    \item[b)] Erneute Anwendung der Tschebyscheffschen Ungleichung liefert nun 
    \[
        \P(|X_n-\E X_n|\geq \tfrac{\varepsilon}{2}) 
        \leq \dfrac{4\Var X_n}{\varepsilon^2} 
        \implies 
        \sum_{n=1}^{\infty} \P(|X_n-\E X_n|\geq \varepsilon) 
        \leq \dfrac{4}{\varepsilon^2} \sum_{n=1}^{\infty} \Var X_n < \infty
    \]
    Das Lemma von Borel-Cantelli sichert nun, dass 
    \[
        \P(\underbrace{\limsup_{n\to\infty} \{\omega \,|\,|X_n(\omega)-\E X_n|\geq \tfrac{\varepsilon}{2}\}}_{=:A}) = 0
    \]
    Das heißt es die Menge aller $\omega\in\Omega$ für welche $|X_n(\omega)-\E X_n|\geq \tfrac{\varepsilon}{2}$ unendlich oft 
    auftritt, ist eine Nullmenge. Demnach gilt für alle anderen $\omega$, dass 
    $|X_n(\omega)-\E X_n|\geq \tfrac{\varepsilon}{2}$ nur endlich oft auftritt, d.h.
    \[
        \forall\omega\in\Omega\backslash A: \exists N_\omega: \forall n\geq N_\omega: 
        |X_n(\omega)-\E X_n| < \tfrac{\varepsilon}{2}
    \]
    Außerdem existiert ein globales $N$, ab welchem $|\E X_n - a | < \tfrac{\varepsilon}{2}$. 
    Für ein festes $\omega\in\Omega\backslash A$ gilt demnach für alle $n\geq \max(N,N_\omega)$:
    \[
        |X_n(\omega) - a| \leq |X_n(\omega) - \E X_n| + |\E X_n - a | 
        < \tfrac{\varepsilon}{2} + \tfrac{\varepsilon}{2} 
        = \varepsilon
    \]
    Also ist $\P(\omega\,|\,X_n(\omega)\to a) = \P(\Omega\backslash A) = 1$
\end{enumerate}
\end{loesung}

\begin{loesung}
\begin{enumerate}
    \item[a)] Für die neuen Zufallsgrößen $X_n$ gilt:
    \[
        \P(|X_n|\geq \varepsilon) 
        = \P(|Y_n\,/\,\ln n|\geq \varepsilon) 
        = \P(Y_n \leq -\varepsilon\ln n) + \P(Y_n \geq \varepsilon\ln n)
        = 0 + e^{-\lambda\varepsilon\ln n}
        = \dfrac{e^{-\lambda \varepsilon}}{n}
    \]
    Damit folgt 
    \[
        \P(|X_n|\geq \varepsilon) \nto 0, 
        \quad \text{aber} \quad 
        \sum_{n=1}^{\infty} \P(|X_n|\geq \varepsilon)
        = e^{-\lambda\varepsilon} \cdot \sum_{n=1}^{\infty} \dfrac{1}{n} = \infty
    \]
    Also $X_n\conP 0$ und $X_n\centernot{\xrightarrow{\ f.\ s.}} 0$ (Lemma von Borel-Cantelli).
    \item[b)] Analog zu a) gilt 
    \[
        \P(|X_n|\geq \varepsilon) 
        = \dfrac{e^{-\lambda \varepsilon}}{n^{\ln n}} 
        \leq \dfrac{e^{-\lambda \varepsilon}}{n^2} \quad\text{für } n\geq 8
    \]
    und so folgt
    \[
        \P(|X_n|\geq \varepsilon) \nto 0 
        \quad \text{und} \quad 
        \sum_{n=1}^{\infty} \P(|X_n|\geq \varepsilon)
        \leq e^{-\lambda\varepsilon}\cdot\sum_{n=1}^{7} \dfrac{1}{n^{\ln n}} 
        + e^{-\lambda\varepsilon} \cdot \sum_{n=8}^{\infty} \dfrac{1}{n^2} < \infty
    \]
    Also $X_n\conP 0$ und $X_n\conFS 0$.
\end{enumerate}
\end{loesung}

\begin{loesung} 
\begin{enumerate}
    \item[a)] Aus dem Lemma von Borel-Cantelli folgt:
    \[
        X_n\conFS X 
        \implies \sum_{n=1}^{\infty} \P(|X_n-X|\geq \varepsilon) < \infty 
        \implies \P(|X_n-X|\geq \varepsilon) < 0 
        \implies X_n\conP X 
    \]
    \item[b)] Die Verteilungsfunktion der konstanten Zufallsgröße $c$ lautet:
    \[
        F_c(x) = \P(c\leq x) = \One_{[c,\infty)}(x)
    \]
    Wenn also punktweise $F_{X_n}\nto F_c$, dann gilt für $\varepsilon>0$:
    \begin{align*}
        \P(|X_n-c|\geq \varepsilon) 
        &= \P(X_n\leq -\varepsilon+c) + \P(X_n\geq \varepsilon+c) 
        = F_{X_n}(c-\varepsilon) + 1 - F_{X_n}(c+\varepsilon) \\
        &\nto \One_{[c,\infty)}(c-\varepsilon) + 1 - \One_{[c,\infty)}(c+\varepsilon) = 0 + 1 - 1 = 0
    \end{align*}
\end{enumerate}
\end{loesung}

\begin{loesung}
Analog zum Beweis von Lemma \ref{lem:markovUng} folgt für $f$ nichtnegativ und nichtfallend, dass
\begin{align*}
    \E[f(Y)] 
    &= \int_{\R} f(y) \diff F_Y(y) 
    \geq \int_{\{y\geq \varepsilon\}} f(y) \diff F_Y(y) \\
    &\geq \int_{\{y\geq \varepsilon\}} f(\varepsilon) \diff F_Y(y) 
    = f(\varepsilon)\cdot\P(Y\geq \varepsilon) \\
    \implies \P(Y\geq \varepsilon) &\leq \dfrac{\E[f(Y)]}{f(\varepsilon)}
\end{align*} 
wenn $\E[f(Y)]$ existiert.
\begin{enumerate}
    \item[a)] Für $f(x)=e^{ax}$ folgt 
    \[
        \P(X\geq \varepsilon) \leq \dfrac{\E[e^{aX}]}{e^{a\varepsilon}}
    \]
    \item[b)] Für $Y=|X-\E X|$ folgt
    \item[] \[
        \P(|X-\E X|\geq \varepsilon) \leq \dfrac{\E[f(|X-\E X|)]}{f(\varepsilon)}
    \]
\end{enumerate}
\end{loesung}

\begin{loesung}
Wir zeigen, dass $\log X_n \conFS 1$. Da die fast sichere Konvergenz unter stetigen Abbildungen 
(hier $x\mapsto \exp x$) invariant ist, folgt daraus die Behauptung. 

Es gilt 
\[
    \log X_n = \dfrac{1}{n}\cdot\sum_{i=1}^{n} -\log Y_i
\]
Für die Hilfsgröße $Z_i = -\log Y_i$ ergibt sich 
\[
    \E Z_1 = \int_{0}^{1} \log y \diff y = -y\log y + y\Big|_{y=0}^1 = 1 
\]
und somit liefert das Starke Gesetz der großen Zahlen, dass 
\[
    \log X_n = \dfrac{1}{n}\cdot\sum_{i=1}^{n} -\log Y_i = \dfrac{1}{n}\cdot\sum_{i=1}^{n} Z_i \conFS \E Z_1 = 1
\]
\end{loesung}

\begin{loesung}
Die skalare Multiplikativität folgt daraus, dass die stochastische und fast sichere Konvergenz unter 
stetigen Abbildungen invariant sind.

Für die Additivität seien $(X_n)$ und $(Y_n)$ Folgen von Zufallsgrößen mit $X_n\conP X$ und $Y_n\conP Y$. 
Wir verwenden, dass 
\[
    P(Z_1+Z_2\geq z) 
    \leq P(Z_1\geq \tfrac{z}{2}\text{ oder }Z_2\geq \tfrac{z}{2}) 
    \leq P(Z_1\geq \tfrac{z}{2}) + P(Z_2\geq \tfrac{z}{2})
\]
und folgern damit
\begin{align*}
    \P(|X_n+Y_n - (X+Y)|\geq \varepsilon) 
    &= \P(|X_n-X| + |Y_n-Y|\geq \varepsilon) \\
    &\leq
    \underbrace{\P(|X_n-X| \geq \tfrac{\varepsilon}{2})}_{\to 0} 
    + \underbrace{\P(|Y_n-Y| \geq \tfrac{\varepsilon}{2})}_{\to 0} \nto 0
\end{align*}
\end{loesung}

\begin{loesung}
Sei $\varepsilon>0$. Wegen $x_n\to x$ existiert ein $N_1\in\N$ so dass $|x_n-x|< \tfrac{\varepsilon}{2}$ für $n\geq N_1$.

Weiter gilt für genau dieses $N_1$, dass $\tfrac{1}{n}\sum_{i=1}^{N_1}(x_n-x)\to 0$ und damit existiert ein weiteres 
$N_2\in\N$ mit $|\tfrac{1}{n}\sum_{i=1}^{N_1}(x_n-x)|\leq \tfrac{\varepsilon}{2}$ für $n\geq N_2$.

Für alle $n\geq\max(N_1,N_2)$ gilt nun
\begin{align*}
    \left|\dfrac{1}{n}\sum_{k=1}^{n} x_n - x\right| 
    &= \left|\dfrac{1}{n}\sum_{k=1}^{n} (x_n - x)\right| 
    = \left|\dfrac{1}{n}\sum_{k=1}^{N_1} (x_n - x) + \dfrac{1}{n}\sum_{k=N_1+1}^{n} (x_n - x)\right|\\
    &\leq \left|\dfrac{1}{n}\sum_{k=1}^{N_1} (x_n - x)\right| + \dfrac{1}{n}\sum_{k=N_1+1}^{n} |x_n - x|
    < \dfrac{\varepsilon}{2} + \dfrac{n-N}{n}\cdot\dfrac{\varepsilon}{2} \leq \varepsilon
\end{align*}
\end{loesung}

\begin{loesung}
\begin{enumerate}
    \item[a)] Die Dichte der Gleichverteilung ist gegeben durch:
    \[
        f_X(x) = \dfrac{1}{2a}\cdot\One_{[-a,a]}(x)
    \]
    Damit ergibt sich für $t\neq 0$:
    \[
        \Psi_X(t) 
        = \int_{-a}^{a} e^{itx}\cdot \dfrac{1}{2a} \diff x 
        = \dfrac{1}{2a}\dfrac{1}{it}\cdot e^{itx}\Big|_{x=-a}^a 
        = \dfrac{e^{-iat}-e^{iat}}{2iat} = \sin(at)
    \]
    und für $t=0$ ist $\Phi_X(0)=1$.
    \item[b)] Die Zweipunktverteilung ist diskret mit $x_1=1, x_2=-1$ und $p_1=p_2=0.5$. Somit ergibt sich 
    \[
        \Psi_Y(t) = \dfrac{e^{it}+e^{-it}}{2} = \cos(t)
    \]
    \item[c)] Die Binomialverteilung ist die Summe von unabhängigen Bernoulliverteilungen 
    $B_1,\dots,B_n\sim\mathrm{Ber}()$. Für diese ergibt sich analog zu b):
    \[
        \Psi_{B_i}(t) = (1-p)+pe^{it}
    \]
    Damit folgt für die Summe:
    \[
        \Psi_Z(t) = \prod_{i=1}^n \Psi_{B_i}(t) = \left(1-p+pe^{it}\right)^n
    \]
\end{enumerate}
\end{loesung}

\begin{loesung}
Ein Poissonverteilung $P$ mit Parameter $\lambda>0$ hat die Verteilungsfunktion 
$e^{-\lambda}\sum_{k=0}^{\lfloor x\rfloor} \dfrac{{\lambda^k}}{k!}$:

Demnach lässt sich der gesuchte Term mit einer Poissonverteilung $S_n$ mit Parameter $n$ darstellen durch:

\[
    e^{-n}\sum_{k=1}^{n} \dfrac{{n^k}}{k!} = e^{-n}\sum_{k=0}^{n} \dfrac{{n^k}}{k!} - e^{-n} = \P(S_n\leq n) - e^{-n}
\]

Zur Verwendung des Zentralen Grenzwertsatzes muss $S_n$ als Summe von anderen Verteilungen geschrieben werden. 
Da die Summe von Poissonverteilungen wieder poissonverteilt ist, 
d.h. für $X\sim\mathrm{Pois}(\lambda_1)$ und $Y\sim\mathrm{Pois}(\lambda_2)$ 
gilt $X+Y\sim\mathrm{Pois}(\lambda_1+\lambda_2)$,
ergibt sich die Wahl $X_1,X_2,\dots\sim\mathrm{Pois}(1)$ mit $S_n=\sum_{i=1}^{n} X_i$.
Damit folgt:

\[
    \lim_{n\to\infty} e^{-n}\sum_{k=1}^{n} \dfrac{{n^k}}{k!} 
    = \lim_{n\to\infty} \P(S_n\leq n) - e^{-n} 
    = \Psi\left(\dfrac{n-\E[S_n]}{\sqrt{D^2[S_n]}}\right) = \Psi\left(\dfrac{n-n}{\sqrt{n}}\right)= \Psi(0) = 0.5
\]
\end{loesung}

\refstepcounter{subsection}

\begin{loesung}
    Offensichtlich schließen sich die Begriffe Lagemaß und Streuungsmaß gegenseitig aus.
    Für das Stichprobenmittel $\phi_1$ gilt:
    \[
        \varphi_1(x_1+c,\dots,x_n+c) 
        = \dfrac{1}{n}\sum_{i=1}^{n}(x_i+c) 
        = \dfrac{1}{n} \sum_{i=1}^{n}x_i + \dfrac{1}{n}\cdot nc 
        = \varphi_1(x_1,\dots,x_n) + c
    \]
    Es ist also ein Lagemaß.

    Für die Stichprobenvarianz gilt:
    \begin{align*}
        \varphi_2(x_1+c,\dots,x_n+c) 
        &= \dfrac{1}{n-1}\sum_{i=1}^{n}(x_i+c-\varphi_1(x_1+c,\dots,x_n+c))^2\\
        &= \dfrac{1}{n-1}\sum_{i=1}^{n}(x_i+c-\varphi_1(x_1,\dots,x_n)-c)^2\\
        &= \dfrac{1}{n-1}\sum_{i=1}^{n}(x_i-\varphi_1(x_1,\dots,x_n))^2
        = \varphi_2(x_1,\dots,x_n) 
    \end{align*}
    Sie ist also ein Streumaß
\end{loesung}

\begin{loesung}
    Für den Erwartungswert gilt:
    \[
        \E[\varphi(X_1,\dots,X_n)] = \sum_{j=1}^{n} \alpha_j \E[X_i] = \mu\cdot\sum_{j=1}^{n}\alpha_j
    \]
    Das heißt, damit $\E[\varphi(X_1,\dots,X_n)]=\mu$, muss $\sum_{j=1}^{n}\alpha_j=1$ gelten. 

    Zur Minimierung der Varianz betrachten wir ein Optimierungsproblem mit Nebenbedingung durch 
    die Methode des Lagrange-Multiplikators. Dabei gilt wegen der Unabhängigkeit der $X_i$, dass 
    \[
        D^2[\varphi(X_1,\dots,X_n)] = \sum_{j=1}^{n}\alpha_j^2 D^2[X_i] = \nu\cdot\sum_{j=1}^{n}\alpha_j^2
    \]
    und somit folgt
    \[
        \Lambda(\alpha_1,\dots,\alpha_n, \lambda) 
        = \nu\cdot\sum_{j=1}^{n}\alpha_j^2 - \lambda\cdot\left(\sum_{j=1}^{n}\alpha_j-1\right)
    \]
    Eine Notwendige Bedingung eines Minimum ist $\nabla \Lambda = 0$, d.h. wir erhalten folgendes Gleichungssystem:
    \begin{align*}
        \dfrac{\partial \Lambda}{\partial \alpha_j}
        = 2\alpha_j - \lambda \stackrel{!}{=} 0 \quad\forall j=1,\dots,n\tag{1}\\
         \dfrac{\partial \Lambda}{\partial \lambda} = -\sum_{j=1}^{n}\alpha_j + 1 \stackrel{!}{=} 0\tag{2}
    \end{align*}
    Aus (1) folgt, dass alle $\alpha_j=\tfrac{\lambda}{2}$ konstant sind. Das eingesetzt in (2) liefert 
    \[
        0 = -\sum_{j=1}^{n}\alpha_j + 1 = -\sum_{j=1}^{n}\dfrac{\lambda}{2} + 1 = -\dfrac{n}{2}\cdot\lambda + 1
        \implies \lambda = \dfrac{2}{n}
    \]
    Demnach muss $\alpha_j=\tfrac{1}{n}$ und wir erhalten 
    \[
        \phi(X_1,\dots,X_n) = \sum_{j=1}^{n} \dfrac{1}{n}\cdot X_j = \overline{X}_n
    \]
    als Extremwert. Dieser ist auch offensichtlich ein Minimum, da die Varianz strikt konvex ist:
    \[
        \nabla^2 D^2[\phi(x_1,\dots,x_n)] = 2\nu I_n
    \]
\end{loesung}

\begin{loesung}
    \begin{enumerate}
        \item 
        Für $Y \sim \Gamma(b,p)$ gilt:

        \begin{align*}
            \Psi_Y(t) 
            &= \E[e^{itY}] 
            = \int_{0}^{\infty} e^{ity} \frac{b^p}{\Gamma(p)} e^{-by} y^{p-1} \diff y 
            = \frac{b^p}{\Gamma(p)} \int_{0}^{\infty} e^{-(b-it)y} y^{p-1} \diff y
        \end{align*}

        Wir substituieren $z = (b-it)y$, also $y = \frac{z}{b-it}$ und $\diff y = \frac{1}{b-it}\diff z$:

        \begin{align*}
            \Psi_Y(t) &=
            \frac{b^p}{\Gamma(p)}\int_{0}^{\infty} e^{-(b-it)y} y^{p-1} \diff y
            = \frac{b^p}{\Gamma(p)}\int_{z=0}^{\infty} e^{-z} \left(\frac{z}{b-it}\right)^{p-1} \frac{1}{b-it} \diff z \\
            &= \frac{b^p}{\Gamma(p)}\cdot\frac{1}{(b-it)^p} \int_{0}^{\infty} e^{-z} z^{p-1} \diff z 
            = \frac{b^p}{\Gamma(p)}\cdot\frac{1}{(b-it)^p}\cdot \Gamma(p)
            = \left(1-\dfrac{it}{b}\right)^{-p}
        \end{align*}
        \item Seien $Y_1\sim \Gamma(b,p1)$ und $Y_2\sim \Gamma(b,p_2)$ unabhängig so gilt:
        \begin{align*}
            \Psi_{Y_1+Y_2}(t) 
            = \Psi_{Y_1}(t)\cdot\Psi_{Y_2}(t) 
            = \left(1-\dfrac{it}{b}\right)^{-p_1}\cdot\left(1-\dfrac{it}{b}\right)^{-p_2}
            = \left(1-\dfrac{it}{b}\right)^{-(p_1+p_2)}
        \end{align*}
        Dies ist wieder die charakteristische Funktion einer Gammaverteilung, also folgt $Y_1+Y_2\sim\Gamma(b,p_1+p_2)$.
    \end{enumerate}
\end{loesung}

\begin{loesung}
    Für die charakteristische Funktion der Zufallsgröße $X_i^2$ gilt:
    \begin{align*}
        \Psi_{X_i^2}(t) 
        = \E[e^{itX}] 
        = \int_{-\infty}^{\infty} e^{itx^2}\cdot\dfrac{1}{\sqrt{2\pi}} e^{-\tfrac{x^2}{2}} \diff x
        = \dfrac{1}{\sqrt{2\pi}}\cdot
        \int_{-\infty}^{\infty} \exp\left(-\left(x\sqrt{\tfrac{1}{2}-it}\right)^2\right) \diff x
    \end{align*}
    Substitution von $z=x\sqrt{\tfrac{1}{2}-it}$ und Verwendung des Gaußschen Integrals 
    $\int_{-\infty}^{\infty}e^{-x^2}\diff x = \sqrt{\pi}$ folgt
    \[
        \Psi_{X_i^2}(t) 
        = \dfrac{1}{\sqrt{2\pi}}\cdot \sqrt{\dfrac{2}{1-2it}} \cdot \int_{-\infty}^{\infty}e^{-z^2}\diff z 
        = \dfrac{1}{\sqrt{1-2it}}
    \]
    Wir erkennen also, dass $X_i^2\sim\Gamma(\tfrac{1}{2},\tfrac{1}{2})$. 
    Damit gilt $U_r\sim\Gamma(\tfrac{1}{2},\tfrac{r}{2})$ und hat die Dichte 
    \[
        f_{U_r}(x) 
        = \dfrac{(\tfrac{1}{2})^{r/2}}{\Gamma(r/2)} e^{-\tfrac{1}{2}x} x^{r/2-1}\One_{(0,\infty)}(x)
        = \dfrac{1}{2^{r/2}\Gamma(r/2)} e^{-x/2} x^{(r-2)/2}\One_{(0,\infty)}(x)
    \]
\end{loesung}

\begin{loesung}
    \begin{enumerate}
        \item
        Wegen der Unabhängigkeit einer Stichprobe gilt 
        \[
            \E[\varphi(X_1,\dots,X_n)] 
            = \E[X_1\cdot X_2] 
            = \E[X_1]\cdot\E[X_2] 
            = p\cdot p 
            = p^2
        \]
        und 
        \begin{align*}
            D^2[\varphi(X_1,\dots,X_n)] 
            &= D^2[X_1\cdot X_2] 
            = \E[X_1^2\cdot X_2^2] - \E[X_1\cdot X_2]^2  \\
            &= \E[X_1^2]\cdot\E[X_2^2] - E[X_1]^2\cdot \E[X_2]^2 
            = p\cdot p - p^2\cdot p^2 
            = p^2(1-p^2)
        \end{align*}
        Sei $f(p) = p^2(1-p^2)$ für $p\in(0,1)$, so gilt 
        \[
            f'(p) = 2p-4p^3 \stackrel{!}{=} 0 \iff p = \sqrt{1/2}
        \]
        und $f''(p) = -4<0$ mit $f(0)=f(1)=0$. Demnach ist $f$ maximal bei $p=\sqrt{1/2}$ und so
        \[
            D^2[\varphi(X_1,\dots,X_n)] \leq \dfrac{1}{4}
        \]
        \item
        Den Erwartungswert von $\varphi_2$ liefert die Turmeigenschaft des bedingten Erwartungswertes:
        \[
            \E[\varphi_2(X_1,\dots,X_n)] 
            = \E[\E[\varphi_1(X_1,\dots,X_n)\,|\, S]] 
            = \E[\varphi_1(X_1,\dots,X_n)] = p^2
        \]
        \item 
        \begin{align*}
            D^2[\varphi_2] 
            &= \E[\varphi_2^2] - \E[\varphi_2]^2 
            = \E[\E[\varphi_1\,|\, S]^2] -  \E[\varphi_1]^2 \\
            &\leq \E[\E[\varphi_1^2\,|\, S]] -  \E[\varphi_1]^2 
            = \E[\varphi_1^2] - \E[\varphi_1]^2
            = D^2[\varphi_1]
        \end{align*}
    \end{enumerate}
\end{loesung}

\begin{loesung}
    Aus der Unabhängigkeit von $X_1,\dots,X_n$ folgt die Unabhängigkeit von $X_1^j,\dots,X_n^j$ und damit 
    liefert das Gesetz der großen Zahlen, dass 
    \[
        \overline{X^j}_n = \dfrac{1}{n}\sum_{i=1}^{n} X_i^j \conFS \E[X^j] \quad \text{für alle }j=0,\dots,k
    \]
    Außerdem folgt aus $\overline{X_n}\conFS \E[X]$, dass $\overline{X_n}^l\conFS \E[X]^l$ für alle $l=0,\dots,k$.

    Da $\overline{X_n}^j$ und $\overline{X^l}_n$ beide fast sicher gegen Konstanten konvergieren, folgt 
    \[
        \overline{X^l}_n\cdot\overline{X_n}^j  \conFS  \E[X^l]\cdot\E[X]^j
    \] 
    Und somit liefert der binomische Lehrsatz:
    \begin{align*}
        \dfrac{1}{n-1}\sum_{i=1}^{n}(X_i-\overline{X}_n)^k 
        &= \dfrac{1}{n-1} \sum_{i=1}^{n} \sum_{j+l=k}\cdot \binom{n}{j} X_i^l\cdot\overline{X_n}^j
        = \dfrac{n}{n-1} \sum_{j+l=k} \binom{n}{j}\cdot \overline{X^l}_n \cdot\overline{X_n}^j \\
        &\conFS \sum_{j+l=k} \binom{n}{j}\cdot \E[X^l]\cdot\E[X]^j 
        = \E\left[\sum_{j+l=k} \binom{n}{j}\cdot X^l\cdot\E[X]^j\right]
        = \E(X-\E[X])^k
    \end{align*}
\end{loesung}

\begin{loesung}
    \begin{enumerate}
        \item Seien $F$ und $G$ Verteilungsfunktionen, so gilt:
        \begin{enumerate}
            \item Da $|F(x)-G(x)|\geq 0$ folgt direkt $d_K(F,G)\geq 0$.
            \item Wenn $d_K(F,G)=0$, dann ist $0$ eine obere Schranke von $|F(x)-G(x)|\leq 0$. Damit folgt $F(x)=G(x)$.
            \item Wenn $F=G$, dann gilt offensichtlich $d_K(F,G)=0$.
            \item Wegen $|F(x)-G(x)| = |G(x)-F(x)|$ folgt $d_K(F,G)=d_K(G,F)$
            \item Die Dreiecksungleichung von Betrag und Supremum liefert
            \begin{align*}
                d_K(F,H) 
                &= \sup |F(x)-H(x)| 
                \leq \sup |F(x)-G(x)| + |G(x)-H(x)| \\ 
                &\leq \sup |F(x)-G(x)| + \sup |G(x)-H(x)|
                = d_K(F,H) + d_K(G,H)
            \end{align*}
        \end{enumerate}
        \item $d_K(F_n,F)\nto 0$ ist äquivalent dazu, dass $F_n$ gleichmäßig gegen $F$ konvergiert. Dies impliziert 
        die punktweise Konvergenz von $F_n$ gegen $F$, was gleichbedeutend zur Konvergenz in Verteilung von $X_n$
        gegen $X$ ist.

        Ein mögliches Gegenbeispiel ist $X_n \sim \mathcal{N}(0,\tfrac{1}{n})$. Hierbei gilt 
        \[
            F_n(x) = \Psi(nx) \nto \begin{cases}
                0, & x<0 \\
                1, & x>1 
            \end{cases}
        \]
        Also konvergiert $F_n(x)$ punktweise gegen $F(x)=\One_{x\geq 1}$ in allen Stetigkeitsstellen von $F$. 
        Demnach konvergiert $X_n$ gegen $X\equiv 0$. Aber $|F_n(0)-F(0)|=\tfrac{1}{2}$ und damit folgt 
        $d_K(F_n,F)\geq \tfrac{1}{2}$.
    \end{enumerate}
\end{loesung}

\begin{loesung}
    Sei $\varepsilon\geq 0$. Nach der Markov Ungleichung gilt 
    \[
        \P\left(\int_{\R} \dfrac{(W_n(t)-F(t))^2}{1+t^2}\diff t \geq \varepsilon\right) 
        \leq \dfrac{1}{\varepsilon}\cdot\E\left[\int_{\R} \dfrac{(W_n(t)-F(t))^2}{1+t^2}\diff t\right]
    \]
    Wegen $\tfrac{|W_n(t)-F(t)|^2}{1+t^2}\geq 0$ darf nach dem Satz von Fubini das Integral und der Erwartungswert 
    getauscht werden und es folgt 
    \[
        \dfrac{1}{\varepsilon}\cdot\int_{\R} \dfrac{|W_n(t)-F(t)|^2}{1+t^2}\diff t 
        = \dfrac{1}{\varepsilon}\cdot\int_{\R} \dfrac{\E(W_n(t)-F(t))^2}{1+t^2} \diff t
    \]
    Nach dem Hauptsatz der Statistik ist $\E(W_n(t)-F(t))^2 = D^2[W_n(t)] = \tfrac{1}{n}F(t)(1-F(t))\leq \tfrac{1}{n}$ und damit
    \[
        \dfrac{1}{\varepsilon}\cdot\int_{\R} \dfrac{\E(W_n(t)-F(t))^2}{1+t^2} \diff t 
        \leq \dfrac{1}{n\cdot\varepsilon}\cdot\int_{\R} \dfrac{1}{1+t^2} \diff t 
        = \dfrac{\pi}{n\cdot\varepsilon} 
        \nto 0
    \]
\end{loesung}

\begin{loesung}
    \begin{enumerate}
        \item Für $T_1$ gilt: 
        \begin{align*}
            F_{T_1}(x) 
            &= \P(T_1 > x) 
            = \P(\min\{X_1,\dots,X_n\} > \tfrac{n}{x}) \\
            &= \P(X_i > \tfrac{n}{x} \text{für alle } i=1,\dots,n) 
            = \P(X_1 > \tfrac{n}{x})^n 
            = (e^{-\tfrac{x}{n\theta}}\One_{(0,\infty)}(x))^n
            = e^{-\tfrac{x}{\theta}}\One_{(0,\infty)}(x)
        \end{align*}
        Das heißt $T_1\sim\mathrm{Exp}(1/\theta)$. Und damit 
        \[
            \E[T_1] = \theta, \quad D^2[T_1] = \theta^2
        \]
        Und da $T_2$ einfach unser empirischer Erwartungswert ist folgt 
        \[
            \E[T_2] = \E[X_1] = \theta, \quad D^2[T_2] = \dfrac{\theta^2}{n}
        \]
        Da die Varianz des 2. Schätzers gegen 0 geht ist dieser zu bevorzugen.
        \item Da $T_1$ von $n$ unabhängig ist und allgemein nicht konstant $\theta$ ist, kann es nicht gegen 
        $\theta$ konvergieren.
    \end{enumerate}
\end{loesung}
% !TeX root = ../script.tex

\newpage
\section{Lösungen}

\refstepcounter{subsection}
\begin{loesung}
Für die Verteilung von $Y_n$ gilt:

\begin{align*}
    F_{Y_n}(y)
    &= \P(Y_n\leq y) 
    = \P(\max\{X_1,\dots,X_n\}\leq y+\ln n) 
    = \P(X_i\leq y+\ln n, \quad\forall i=1,\dots,n)  \\
    &= \P(X_1\leq y+\ln n)^n
    = \One_{(0,\infty)}(y)\cdot(1-e^{-(y+\ln n)})^n
    = \One_{(0,\infty)}(y)\cdot\left(1-\dfrac{e^{-y}}{n}\right)^n
\end{align*}

Diese Verteilung ist überall stetig und für den Grenzwert gilt

\[
    \lim_{n\to\infty} F_{Y_n}(y) = \One_{(0,\infty)}(y)\cdot\exp(-e^{-y})
\]
\end{loesung}

\begin{loesung}
\begin{enumerate}
    \item[a)] Aus der Tschebyscheffschen Ungleichung folgt für $p=2$, dass 
    \begin{align*}  
        \P(|X_n - a| \geq \varepsilon) 
        &\leq \dfrac{\E(X_n-a)^2}{\varepsilon^2}  \\
        &= \dfrac{\E(X_n)^2-2a\E X_n+a^2}{\varepsilon^2} \\
        &= \dfrac{(\E X_n)^2-\Var X_n - 2a\E X_n+a^2}{\varepsilon^2} \\
        &\to \dfrac{a^2-0-2a^2+a^2}{\varepsilon^2} = 0
    \end{align*}
    und damit folgt $X_n\conP a$.
    \item[b)] Erneute Anwendung der Tschebyscheffschen Ungleichung liefert nun 
    \[
        \P(|X_n-\E X_n|\geq \tfrac{\varepsilon}{2}) 
        \leq \dfrac{4\Var X_n}{\varepsilon^2} 
        \implies 
        \sum_{n=1}^{\infty} \P(|X_n-\E X_n|\geq \varepsilon) 
        \leq \dfrac{4}{\varepsilon^2} \sum_{n=1}^{\infty} \Var X_n < \infty
    \]
    Das Lemma von Borel-Cantelli sichert nun, dass 
    \[
        \P(\underbrace{\limsup_{n\to\infty} \{\omega \,|\,|X_n(\omega)-\E X_n|\geq \tfrac{\varepsilon}{2}\}}_{=:A}) = 0
    \]
    Das heißt es die Menge aller $\omega\in\Omega$ für welche $|X_n(\omega)-\E X_n|\geq \tfrac{\varepsilon}{2}$ unendlich oft 
    auftritt, ist eine Nullmenge. Demnach gilt für alle anderen $\omega$, dass 
    $|X_n(\omega)-\E X_n|\geq \tfrac{\varepsilon}{2}$ nur endlich oft auftritt, d.h.
    \[
        \forall\omega\in\Omega\backslash A: \exists N_\omega: \forall n\geq N_\omega: 
        |X_n(\omega)-\E X_n| < \tfrac{\varepsilon}{2}
    \]
    Außerdem existiert ein globales $N$, ab welchem $|\E X_n - a | < \tfrac{\varepsilon}{2}$. 
    Für ein festes $\omega\in\Omega\backslash A$ gilt demnach für alle $n\geq \max(N,N_\omega)$:
    \[
        |X_n(\omega) - a| \leq |X_n(\omega) - \E X_n| + |\E X_n - a | 
        < \tfrac{\varepsilon}{2} + \tfrac{\varepsilon}{2} 
        = \varepsilon
    \]
    Also ist $\P(\omega\,|\,X_n(\omega)\to a) = \P(\Omega\backslash A) = 1$
\end{enumerate}
\end{loesung}

\begin{loesung}
\begin{enumerate}
    \item[a)] Für die neuen Zufallsgrößen $X_n$ gilt:
    \[
        \P(|X_n|\geq \varepsilon) 
        = \P(|Y_n\,/\,\ln n|\geq \varepsilon) 
        = \P(Y_n \leq -\varepsilon\ln n) + \P(Y_n \geq \varepsilon\ln n)
        = 0 + e^{-\lambda\varepsilon\ln n}
        = \dfrac{e^{-\lambda \varepsilon}}{n}
    \]
    Damit folgt 
    \[
        \P(|X_n|\geq \varepsilon) \nto 0, 
        \quad \text{aber} \quad 
        \sum_{n=1}^{\infty} \P(|X_n|\geq \varepsilon)
        = e^{-\lambda\varepsilon} \cdot \sum_{n=1}^{\infty} \dfrac{1}{n} = \infty
    \]
    Also $X_n\conP 0$ und $X_n\centernot{\xrightarrow{\ f.\ s.}} 0$ (Lemma von Borel-Cantelli).
    \item[b)] Analog zu a) gilt 
    \[
        \P(|X_n|\geq \varepsilon) 
        = \dfrac{e^{-\lambda \varepsilon}}{n^{\ln n}} 
        \leq \dfrac{e^{-\lambda \varepsilon}}{n^2} \quad\text{für } n\geq 8
    \]
    und so folgt
    \[
        \P(|X_n|\geq \varepsilon) \nto 0 
        \quad \text{und} \quad 
        \sum_{n=1}^{\infty} \P(|X_n|\geq \varepsilon)
        \leq e^{-\lambda\varepsilon}\cdot\sum_{n=1}^{7} \dfrac{1}{n^{\ln n}} 
        + e^{-\lambda\varepsilon} \cdot \sum_{n=8}^{\infty} \dfrac{1}{n^2} < \infty
    \]
    Also $X_n\conP 0$ und $X_n\conFS 0$.
\end{enumerate}
\end{loesung}

\begin{loesung} 
\begin{enumerate}
    \item[a)] Aus dem Lemma von Borel-Cantelli folgt:
    \[
        X_n\conFS X 
        \implies \sum_{n=1}^{\infty} \P(|X_n-X|\geq \varepsilon) < \infty 
        \implies \P(|X_n-X|\geq \varepsilon) < 0 
        \implies X_n\conP X 
    \]
    \item[b)] Die Verteilungsfunktion der konstanten Zufallsgröße $c$ lautet:
    \[
        F_c(x) = \P(c\leq x) = \One_{[c,\infty)}(x)
    \]
    Wenn also punktweise $F_{X_n}\nto F_c$, dann gilt für $\varepsilon>0$:
    \begin{align*}
        \P(|X_n-c|\geq \varepsilon) 
        &= \P(X_n\leq -\varepsilon+c) + \P(X_n\geq \varepsilon+c) 
        = F_{X_n}(c-\varepsilon) + 1 - F_{X_n}(c+\varepsilon) \\
        &\nto \One_{[c,\infty)}(c-\varepsilon) + 1 - \One_{[c,\infty)}(c+\varepsilon) = 0 + 1 - 1 = 0
    \end{align*}
\end{enumerate}
\end{loesung}

\begin{loesung}
Analog zum Beweis von Lemma \ref{lem:markovUng} folgt für $f$ nichtnegativ und nichtfallend, dass
\begin{align*}
    \E[f(Y)] 
    &= \int_{\R} f(y) \diff F_Y(y) 
    \geq \int_{\{y\geq \varepsilon\}} f(y) \diff F_Y(y) \\
    &\geq \int_{\{y\geq \varepsilon\}} f(\varepsilon) \diff F_Y(y) 
    = f(\varepsilon)\cdot\P(Y\geq \varepsilon) \\
    \implies \P(Y\geq \varepsilon) &\leq \dfrac{\E[f(Y)]}{f(\varepsilon)}
\end{align*} 
wenn $\E[f(Y)]$ existiert.
\begin{enumerate}
    \item[a)] Für $f(x)=e^{ax}$ folgt 
    \[
        \P(X\geq \varepsilon) \leq \dfrac{\E[e^{aX}]}{e^{a\varepsilon}}
    \]
    \item[b)] Für $Y=|X-\E X|$ folgt
    \item[] \[
        \P(|X-\E X|\geq \varepsilon) \leq \dfrac{\E[f(|X-\E X|)]}{f(\varepsilon)}
    \]
\end{enumerate}
\end{loesung}

\begin{loesung}
Wir zeigen, dass $\log X_n \conFS 1$. Da die fast sichere Konvergenz unter stetigen Abbildungen 
(hier $x\mapsto \exp x$) invariant ist, folgt daraus die Behauptung. 

Es gilt 
\[
    \log X_n = \dfrac{1}{n}\cdot\sum_{i=1}^{n} -\log Y_i
\]
Für die Hilfsgröße $Z_i = -\log Y_i$ ergibt sich 
\[
    \E Z_1 = \int_{0}^{1} \log y \diff y = -y\log y + y\Big|_{y=0}^1 = 1 
\]
und somit liefert das Starke Gesetz der großen Zahlen, dass 
\[
    \log X_n = \dfrac{1}{n}\cdot\sum_{i=1}^{n} -\log Y_i = \dfrac{1}{n}\cdot\sum_{i=1}^{n} Z_i \conFS \E Z_1 = 1
\]
\end{loesung}

\begin{loesung}
Die skalare Multiplikativität folgt daraus, dass die stochastische und fast sichere Konvergenz unter 
stetigen Abbildungen invariant sind.

Für die Additivität seien $(X_n)$ und $(Y_n)$ Folgen von Zufallsgrößen mit $X_n\conP X$ und $Y_n\conP Y$. 
Wir verwenden, dass 
\[
    P(Z_1+Z_2\geq z) 
    \leq P(Z_1\geq \tfrac{z}{2}\text{ oder }Z_2\geq \tfrac{z}{2}) 
    \leq P(Z_1\geq \tfrac{z}{2}) + P(Z_2\geq \tfrac{z}{2})
\]
und folgern damit
\begin{align*}
    \P(|X_n+Y_n - (X+Y)|\geq \varepsilon) 
    &= \P(|X_n-X| + |Y_n-Y|\geq \varepsilon) \\
    &\leq
    \underbrace{\P(|X_n-X| \geq \tfrac{\varepsilon}{2})}_{\to 0} 
    + \underbrace{\P(|Y_n-Y| \geq \tfrac{\varepsilon}{2})}_{\to 0} \nto 0
\end{align*}
\end{loesung}

\begin{loesung}
Sei $\varepsilon>0$. Wegen $x_n\to x$ existiert ein $N_1\in\N$ so dass $|x_n-x|< \tfrac{\varepsilon}{2}$ für $n\geq N_1$.

Weiter gilt für genau dieses $N_1$, dass $\tfrac{1}{n}\sum_{i=1}^{N_1}(x_n-x)\to 0$ und damit existiert ein weiteres 
$N_2\in\N$ mit $|\tfrac{1}{n}\sum_{i=1}^{N_1}(x_n-x)|\leq \tfrac{\varepsilon}{2}$ für $n\geq N_2$.

Für alle $n\geq\max(N_1,N_2)$ gilt nun
\begin{align*}
    \left|\dfrac{1}{n}\sum_{k=1}^{n} x_n - x\right| 
    &= \left|\dfrac{1}{n}\sum_{k=1}^{n} (x_n - x)\right| 
    = \left|\dfrac{1}{n}\sum_{k=1}^{N_1} (x_n - x) + \dfrac{1}{n}\sum_{k=N_1+1}^{n} (x_n - x)\right|\\
    &\leq \left|\dfrac{1}{n}\sum_{k=1}^{N_1} (x_n - x)\right| + \dfrac{1}{n}\sum_{k=N_1+1}^{n} |x_n - x|
    < \dfrac{\varepsilon}{2} + \dfrac{n-N}{n}\cdot\dfrac{\varepsilon}{2} \leq \varepsilon
\end{align*}
\end{loesung}

\begin{loesung}
\begin{enumerate}
    \item[a)] Die Dichte der Gleichverteilung ist gegeben durch:
    \[
        f_X(x) = \dfrac{1}{2a}\cdot\One_{[-a,a]}(x)
    \]
    Damit ergibt sich für $t\neq 0$:
    \[
        \Psi_X(t) 
        = \int_{-a}^{a} e^{itx}\cdot \dfrac{1}{2a} \diff x 
        = \dfrac{1}{2a}\dfrac{1}{it}\cdot e^{itx}\Big|_{x=-a}^a 
        = \dfrac{e^{-iat}-e^{iat}}{2iat} = \sin(at)
    \]
    und für $t=0$ ist $\Phi_X(0)=1$.
    \item[b)] Die Zweipunktverteilung ist diskret mit $x_1=1, x_2=-1$ und $p_1=p_2=0.5$. Somit ergibt sich 
    \[
        \Psi_Y(t) = \dfrac{e^{it}+e^{-it}}{2} = \cos(t)
    \]
    \item[c)] Die Binomialverteilung ist die Summe von unabhängigen Bernoulliverteilungen 
    $B_1,\dots,B_n\sim\mathrm{Ber}()$. Für diese ergibt sich analog zu b):
    \[
        \Psi_{B_i}(t) = (1-p)+pe^{it}
    \]
    Damit folgt für die Summe:
    \[
        \Psi_Z(t) = \prod_{i=1}^n \Psi_{B_i}(t) = \left(1-p+pe^{it}\right)^n
    \]
\end{enumerate}
\end{loesung}

\begin{loesung}
Ein Poissonverteilung $P$ mit Parameter $\lambda>0$ hat die Verteilungsfunktion 
$e^{-\lambda}\sum_{k=0}^{\lfloor x\rfloor} \dfrac{{\lambda^k}}{k!}$:

Demnach lässt sich der gesuchte Term mit einer Poissonverteilung $S_n$ mit Parameter $n$ darstellen durch:

\[
    e^{-n}\sum_{k=1}^{n} \dfrac{{n^k}}{k!} = e^{-n}\sum_{k=0}^{n} \dfrac{{n^k}}{k!} - e^{-n} = \P(S_n\leq n) - e^{-n}
\]

Zur Verwendung des Zentralen Grenzwertsatzes muss $S_n$ als Summe von anderen Verteilungen geschrieben werden. 
Da die Summe von Poissonverteilungen wieder poissonverteilt ist, 
d.h. für $X\sim\mathrm{Pois}(\lambda_1)$ und $Y\sim\mathrm{Pois}(\lambda_2)$ 
gilt $X+Y\sim\mathrm{Pois}(\lambda_1+\lambda_2)$,
ergibt sich die Wahl $X_1,X_2,\dots\sim\mathrm{Pois}(1)$ mit $S_n=\sum_{i=1}^{n} X_i$.
Damit folgt:

\[
    \lim_{n\to\infty} e^{-n}\sum_{k=1}^{n} \dfrac{{n^k}}{k!} 
    = \lim_{n\to\infty} \P(S_n\leq n) - e^{-n} 
    = \Psi\left(\dfrac{n-\E[S_n]}{\sqrt{D^2[S_n]}}\right) = \Psi\left(\dfrac{n-n}{\sqrt{n}}\right)= \Psi(0) = 0.5
\]
\end{loesung}

\refstepcounter{subsection}

\begin{loesung}
    Offensichtlich schließen sich die Begriffe Lagemaß und Streuungsmaß gegenseitig aus.
    Für das Stichprobenmittel $\phi_1$ gilt:
    \[
        \varphi_1(x_1+c,\dots,x_n+c) 
        = \dfrac{1}{n}\sum_{i=1}^{n}(x_i+c) 
        = \dfrac{1}{n} \sum_{i=1}^{n}x_i + \dfrac{1}{n}\cdot nc 
        = \varphi_1(x_1,\dots,x_n) + c
    \]
    Es ist also ein Lagemaß.

    Für die Stichprobenvarianz gilt:
    \begin{align*}
        \varphi_2(x_1+c,\dots,x_n+c) 
        &= \dfrac{1}{n-1}\sum_{i=1}^{n}(x_i+c-\varphi_1(x_1+c,\dots,x_n+c))^2\\
        &= \dfrac{1}{n-1}\sum_{i=1}^{n}(x_i+c-\varphi_1(x_1,\dots,x_n)-c)^2\\
        &= \dfrac{1}{n-1}\sum_{i=1}^{n}(x_i-\varphi_1(x_1,\dots,x_n))^2
        = \varphi_2(x_1,\dots,x_n) 
    \end{align*}
    Sie ist also ein Streumaß
\end{loesung}

\begin{loesung}
    Für den Erwartungswert gilt:
    \[
        \E[\varphi(X_1,\dots,X_n)] = \sum_{j=1}^{n} \alpha_j \E[X_i] = \mu\cdot\sum_{j=1}^{n}\alpha_j
    \]
    Das heißt, damit $\E[\varphi(X_1,\dots,X_n)]=\mu$, muss $\sum_{j=1}^{n}\alpha_j=1$ gelten. 

    Zur Minimierung der Varianz betrachten wir ein Optimierungsproblem mit Nebenbedingung durch 
    die Methode des Lagrange-Multiplikators. Dabei gilt wegen der Unabhängigkeit der $X_i$, dass 
    \[
        D^2[\varphi(X_1,\dots,X_n)] = \sum_{j=1}^{n}\alpha_j^2 D^2[X_i] = \nu\cdot\sum_{j=1}^{n}\alpha_j^2
    \]
    und somit folgt
    \[
        \Lambda(\alpha_1,\dots,\alpha_n, \lambda) 
        = \nu\cdot\sum_{j=1}^{n}\alpha_j^2 - \lambda\cdot\left(\sum_{j=1}^{n}\alpha_j-1\right)
    \]
    Eine Notwendige Bedingung eines Minimum ist $\nabla \Lambda = 0$, d.h. wir erhalten folgendes Gleichungssystem:
    \begin{align*}
        \dfrac{\partial \Lambda}{\partial \alpha_j}
        = 2\alpha_j - \lambda \stackrel{!}{=} 0 \quad\forall j=1,\dots,n\tag{1}\\
         \dfrac{\partial \Lambda}{\partial \lambda} = -\sum_{j=1}^{n}\alpha_j + 1 \stackrel{!}{=} 0\tag{2}
    \end{align*}
    Aus (1) folgt, dass alle $\alpha_j=\tfrac{\lambda}{2}$ konstant sind. Das eingesetzt in (2) liefert 
    \[
        0 = -\sum_{j=1}^{n}\alpha_j + 1 = -\sum_{j=1}^{n}\dfrac{\lambda}{2} + 1 = -\dfrac{n}{2}\cdot\lambda + 1
        \implies \lambda = \dfrac{2}{n}
    \]
    Demnach muss $\alpha_j=\tfrac{1}{n}$ und wir erhalten 
    \[
        \phi(X_1,\dots,X_n) = \sum_{j=1}^{n} \dfrac{1}{n}\cdot X_j = \overline{X}_n
    \]
    als Extremwert. Dieser ist auch offensichtlich ein Minimum, da die Varianz strikt konvex ist:
    \[
        \nabla^2 D^2[\phi(x_1,\dots,x_n)] = 2\nu I_n
    \]
\end{loesung}

\begin{loesung}
    \begin{enumerate}
        \item 
        Für $Y \sim \Gamma(b,p)$ gilt:

        \begin{align*}
            \Psi_Y(t) 
            &= \E[e^{itY}] 
            = \int_{0}^{\infty} e^{ity} \frac{b^p}{\Gamma(p)} e^{-by} y^{p-1} \diff y 
            = \frac{b^p}{\Gamma(p)} \int_{0}^{\infty} e^{-(b-it)y} y^{p-1} \diff y
        \end{align*}

        Wir substituieren $z = (b-it)y$, also $y = \frac{z}{b-it}$ und $\diff y = \frac{1}{b-it}\diff z$:

        \begin{align*}
            \Psi_Y(t) &=
            \frac{b^p}{\Gamma(p)}\int_{0}^{\infty} e^{-(b-it)y} y^{p-1} \diff y
            = \frac{b^p}{\Gamma(p)}\int_{z=0}^{\infty} e^{-z} \left(\frac{z}{b-it}\right)^{p-1} \frac{1}{b-it} \diff z \\
            &= \frac{b^p}{\Gamma(p)}\cdot\frac{1}{(b-it)^p} \int_{0}^{\infty} e^{-z} z^{p-1} \diff z 
            = \frac{b^p}{\Gamma(p)}\cdot\frac{1}{(b-it)^p}\cdot \Gamma(p)
            = \left(1-\dfrac{b}{it}\right)^p
        \end{align*}
        \item Seien $Y_1\sim \Gamma(b,p1)$ und $Y_2\sim \Gamma(b,p_2)$ unabhängig so gilt:
        \begin{align*}
            \Psi_{Y_1+Y_2}(t) 
            = \Psi_{Y_1}(t)\cdot\Psi_{Y_2}(t) 
            = \left(1-\dfrac{b}{it}\right)^{p_1}\cdot\left(1-\dfrac{b}{it}\right)^{p_2}
            = \left(1-\dfrac{b}{it}\right)^{p_1+p_2}
        \end{align*}
        Dies ist wieder die charakteristische Funktion einer Gammaverteilung, also folgt $Y_1+Y_2\sim\Gamma(b,p_1+p_2)$.
    \end{enumerate}
\end{loesung}

\begin{loesung}
    Für die charakteristische Funktion der Zufallsgröße $X_i^2$ gilt:
    \begin{align*}
        \Psi_{X_i^2}(t) 
        = \E[e^{itX}] 
        = \int_{0}^{\infty} e^{itx^2}\cdot\dfrac{1}{\sqrt{2\pi}} e^{-\tfrac{x^2}{2}} \diff x
        = \dfrac{1}{\sqrt{2\pi}}\cdot\int_{0}^{\infty} e^{-(x(\tfrac{1}{2}-it))^2} \diff x
    \end{align*}
    Substitution von $z=x(\tfrac{1}{2}-it)$ und Verwendung des Gaußschen Integrals 
    $\int_{0}^{\infty}e^{-x^2}\diff x = \tfrac{\sqrt{\pi}}{2}$ folgt
    \[
        \Psi_{X_i^2}(t) = \dfrac{1}{\sqrt{2\pi}}\cdot
    \]
\end{loesung}